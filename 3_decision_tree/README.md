## Decision Tree 决策树

核心问题：
- 如何构建一颗决策树？
- 构建决策树的依据是什么？

### 2. 构建一棵树决策树的过程

名词：
- 训练集
- 属性集
- 分支结点：代表数据/样本的类别

构建过程:
通过一个递归函数来为每个结点找到最合适属性集，直到结束条件出现:
- 所有节点都属于同一属性类型，无需划分

### 2.1 构建决策树的核心问题

选择最优划分属性，也就是让分支结点所包含的样本尽可能属于同一类，即结点的“纯度”高。

通俗的理解最优属性就是包含最多样本的属性，每次都选择这样的属性，就能最先把尽量多的样本分类出去。

这样做是不是太简单了？数学家们肯定不会用这么low的办法。于是，他们搞出了个“信息增益”的概念。

某个属性的"信息增益"越大，就应该首先使用它来作为决策树的结点。

先了解信息熵的概念。

#### 2.1.1 信息熵Information Entropy

怎么量化“纯度” ？

信息熵定义描述：假定当前集合D中第k类样本所占的比例为
![](http://latex.codecogs.com/png.latex?p_{k})k=1,2,...,|y|, 则D的信息熵定义为:

![](http://latex.codecogs.com/png.latex?Ent(D)=-\sum_{k=1}^{|y|}p_{k}log_{2}^{p_{k}})

注意：
- 这是计算集合D的信息熵
- |y|代表的是集合D的元素类别个数, 注意不是元素个数
- 为什么要给连加号前加个负号？
  - ![](http://latex.codecogs.com/png.latex?p_{k})必定小于或等于1,所以![](http://latex.codecogs.com/png.latex?log_{2}^{k})必定小于或等于0.
  - 为了让Ent(D)大于0就加上了负号.
- Ent(D)的极值
  - 最小值 0: 当![](http://latex.codecogs.com/png.latex?p_{k})等于0时
  - 最大值 ![](http://latex.codecogs.com/png.latex?log_{2}^{|y|})
    - 条件是所有集合D中所有类别的样本数量相等，也就是说![](http://latex.codecogs.com/png.latex?p_{k})等于1/|y|.
    - 取最大值的意义是：D中所有类别包含的样本数量一样多，这种情况样本混合度最高，纯度也是最小


#### 2.1.2 信息增益Information Gain

信息增益的定义:

![](http://latex.codecogs.com/png.latex?Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{|D^{v}|}{|D|}Ent(D^{v}))

参数说明:
- a是离散属性
- a属性包含V个可能的取值![](http://latex.codecogs.com/png.latex?{a^{1},a^{2},...,a^{V}}) 例如：颜色属性可能的取值为{红色, 绿色, ...}
- r 如果使用属性a 对样本集进行划分， 就会产生V个分支结点， 其中第v个分支结点包含了 ![](http://latex.codecogs.com/png.latex?D^{v}) 个 样本，这些样本的属性取值都是![](http://latex.codecogs.com/png.latex?{a^{v}})
- ![](http://latex.codecogs.com/png.latex?|D^{v}|)是D中包含分类为a的第v类属性的样本数量
- ![](http://latex.codecogs.com/png.latex?Ent(D^{v}))的意义是计算样本![](http://latex.codecogs.com/png.latex?D^{v})的信息熵，此时信息熵公式中的|y|=1，就是说只有一类样本

意义说明:
- 这是求属性a的信息增益
- 信息增益越大, 意味着使用属性a来划分样本所获得的"纯度提升"越大


#### 2.1.3 以西瓜数据集为例来构建决策树

- 信息熵公式中|y|=2，意思是西瓜分类只有好瓜和坏瓜两种
- 计算出好瓜和坏瓜的数量就可以计算出Ent(D)

### 2.2 构建决策树的方法

- ID3
- C4.5
- CART决策树

上边使用信息增益来选择属性的方法是`ID3`.

使用`增益率`来作为属性选择的标准的方法叫做`C4.5`.

增益率定义:

![](http://latex.codecogs.com/png.latex?Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)})

其中,

![](http://latex.codecogs.com/png.latex?IV(a)=-\sum_{v=1}^{V}\frac{|D^{v}|}{|D|}\log_{2}\frac{|D^{v}|}{|D|})

为什么要使用增益率来替代信息增益？
- 信息增益对可取值较多的属性有所偏好，就是说这种属性的信息增益值较大。
- 增益率对属性就没有偏好吗？
  - 有，对可取值较少的属性有偏好

 所以C4.5并不是直接使用增益率，而是: 先从候选属性中找到信息增益高于平均的属性，然后再找其中选择增益率最大的。

使用`基尼指数`选择属性的方法叫`CART决策树`.

基尼指数Gini Index的计算公式:

![](http://latex.codecogs.com/png.latex?GiniIndex(D,a)=\sum_{v=1}^{V}\frac{|D^{v}|}{|D|}Gini(D^{v}))

其中![](http://latex.codecogs.com/png.latex?Gini(D^{v}))的定义：

![](http://latex.codecogs.com/png.latex?Gini(D)=\sum_{k\ =\ 1}^{|u|}\sum_{{k}\ \neq\ k'}{}p_{k}p_{{k}'}=1-\sum_{k\ =\ 1}^{|y|}p_{k}^{2})

![](http://latex.codecogs.com/png.latex?Gini(D))的含义:
- 衡量数据集D的纯度, 纯度越高 说明更多样本有相同的属性
- 反映了从数据集D中随机抽取两个样本，两个样本不属于同一属性的概率；概率跟纯度成反比
- 基尼指数越小的属性，越适合用来划分样本

### 3. 关于剪枝pruning处理

剪枝解决什么问题？
- 决策树的过拟合问题
- 怎么衡量剪枝的效果：提升决策树的泛化能力

剪枝的方法:
- 预剪枝pre-pruning
  -  生成决策树过程中，预先估计当前结点 能不能提升决策树泛化能力，如果能就保留, 不能就停止划分并将该结点标记为叶结点
- 后剪枝post-pruning
  - 生成决策树后， 自底而上的检查每个非叶结点，如果将该结点替换为叶结点能提升泛化能力就替换。

-------

以下的内容只是简单了解了一下，没有做公式推导。

-------

### 4. 处理连续属性和缺失值的情况

以上的讨论都是基于离散属性的，怎么处理连续属性？

#### 4.1 连续属性

核心方法：把连续属性的取值处理成离散值, 按照离散属性的计算方法来处理。

#### 4.2 缺失值处理

### 5. 多变量决策树

核心思想：使用属性的线性组合替代单个属性来作为非叶结点; 创建线性分类器。

形象的理解这个变化：
- 把每个属性想象成坐标空间的一个坐标轴,d 个属性就代表d维 空间坐标
- 每个样本点就是d维坐标空间中的点
- 不同(分类)样本点的分类边界 是跟坐标轴平行的, 因此是个多线段组成的分类边界线
- 多变量决策树可以把多线段边界线处理成平滑曲线分界线
